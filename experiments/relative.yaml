experiment_name: rotary_positional_encoding
status: completed

objective: >
  Evaluate the effect of learned (trainable) positional embeddings on
  Transformer optimization behavior in a low-resource machine translation
  setting.

dataset:
  name: OPUS Books
  language_pair: de-en
  train_samples: 2000
  validation_samples: 500
  tokenization: Helsinki-NLP/opus-mt-de-en

model:
  architecture: Transformer (encoder-decoder)
  implementation: torch.nn.Transformer
  d_model: 512
  nhead: 8
  encoder_layers: 4
  decoder_layers: 4
  feedforward_dim: 2048
  dropout: 0.1

positional_encoding:
  type: rotary
  trainable: false
  relative: true

training:
  epochs: 3
  batch_size: 32
  optimizer: AdamW
  learning_rate: 0.0003
  loss_function: CrossEntropyLoss
  ignore_padding_index: true
  device: cuda

evaluation:
  metric: cross_entropy_loss
  evaluation_strategy: validation_loss_per_epoch
  bleu_used: false
  reason_bleu_not_used: >
    BLEU scores were unstable for scratch-trained models under
    extremely low-resource conditions.

results:
  final_train_loss: 5.8420
  final_validation_loss: 5.8576
  convergence_behavior: stable
  overfitting_observed: no

artifacts:
  loss_curve: results/plots/learned_loss.png
  loss_log: results/logs/learned_losses.json

notes:
  - Rotary encoding leads to slower convergence under low-resource training.
  - Relative positional bias does not help early optimization in this regime.