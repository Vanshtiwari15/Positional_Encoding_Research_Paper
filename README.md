# ğŸ“ Positional Encoding in Low-Resource Transformer Training

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)](https://pytorch.org/)
[![Transformer](https://img.shields.io/badge/Model-Encoder--Decoder-orange.svg)]()
[![Research](https://img.shields.io/badge/Type-Empirical%20Study-purple.svg)]()
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)]()

> A controlled empirical study analyzing how different positional encoding strategies affect Transformer optimization under extreme low-resource training conditions.

---

## ğŸŒŸ Overview

Transformers require positional encodings to model token order. Most research evaluates these mechanisms in large-scale settings.  

This project studies their behavior under **extreme low-resource conditions (2,000 training examples)**.

We implement a standard encoderâ€“decoder Transformer from scratch and compare:

- Sinusoidal Positional Encoding
- Learned Absolute Positional Embeddings
- Rotary Positional Encoding (RoPE)

All experiments keep architecture and hyperparameters identical.

---

## ğŸ”¬ Research Question

Does increasing positional encoding complexity improve optimization when data is severely limited?

---

## ğŸ§  Experimental Setup

### ğŸ“š Dataset
- OPUS Books (Germanâ€“English)
- 2,000 sentence pairs
- Max sequence length: 64
- Marian tokenizer (Helsinki-NLP/opus-mt-de-en)

### âš™ï¸ Model Configuration
- Encoderâ€“Decoder Transformer
- 4 layers (encoder & decoder)
- Model dimension: 512
- 8 attention heads
- Feedforward dimension: 2048
- Dropout: 0.1
- Optimizer: AdamW
- Learning rate: 3e-4
- Batch size: 32
- Epochs: 3

### ğŸ“Š Evaluation
- Training Cross-Entropy Loss
- Validation Cross-Entropy Loss
- BLEU not used due to instability in low-resource regime

---

## ğŸ“ˆ Results

| Positional Encoding | Train Loss | Validation Loss |
|---------------------|------------|-----------------|
| Sinusoidal          | 5.8132     | 5.8166          |
| Learned             | 5.8284     | 5.8113          |
| Rotary              | 5.8420     | 5.8576          |

### Key Findings

- Learned embeddings show no advantage over sinusoidal encoding.
- Rotary encoding converges more slowly in low-resource settings.
- Simpler encodings are sufficient under extreme data constraints.

---

## ğŸ—ï¸ Architecture

The model follows a standard encoderâ€“decoder Transformer architecture.

### ğŸ”¹ Input Processing
Input Sentence (German)
â†“
Tokenization
â†“
Token Embeddings
â†“

Positional Encoding (Sinusoidal / Learned / Rotary)


Maximum sequence length (context window): **64 tokens**

All tokens within this context window attend to each other via self-attention.

---

### ğŸ”¹ Encoder
[ Self-Attention ]
â†“
[ Add & LayerNorm ]
â†“
[ Feedforward Network ]
â†“
[ Add & LayerNorm ]


- 4 stacked encoder layers  
- Model dimension: 512  
- 8 attention heads  
- Full attention within the 64-token context window  

Each token attends to all other tokens in the input sequence.

---

### ğŸ”¹ Decoder
Target Tokens (shifted right)
â†“
Masked Self-Attention (causal)
â†“
Cross-Attention (attends to encoder outputs)
â†“
Feedforward Network


- 4 stacked decoder layers  
- Causal masking ensures tokens only attend to previous positions  
- Cross-attention connects decoder to encoder representations  

---

### ğŸ”¹ Context Window Behavior

- Maximum sequence length: **64**
- Attention complexity: **O(nÂ²)** within the window
- No sparse or sliding-window attention
- Rotary encoding (when used) modifies query/key vectors inside attention

---

### ğŸ”¹ Output



Decoder Hidden States
â†“
Linear Projection
â†“
Softmax
â†“
Next-Token Probability Distribution


Training objective: **Cross-Entropy Loss**

---

### ğŸ”¬ Positional Encoding Injection

- Sinusoidal / Learned â†’ added to token embeddings
- Rotary â†’ applied directly inside attention mechanism

All other architectural components remain identical across experiments.
